{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repaso de Álgebra Lineal\n",
    "### Aprendizaje Automático - Instituto de Computación - UdelaR\n",
    "\n",
    "A continuación, presentaremos algunos conceptos básicos y notación para el trabajo con vectores y matrices. Está basado en el artículo \"[Linear Algebra Review and Reference](https://see.stanford.edu/materials/aimlcs229/cs229-linalg.pdf)\", del curso CS229 de Stanford (que sugerimos leer). Para tener una excelente visión intuitiva sobre qué es el álgebra lineal y su vínculo con la geometría, sugerimos la serie de videos \"[Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\", de 3BLUE1BROWN.\n",
    "\n",
    "\n",
    "Considere el siguiente sistema de ecuaciones lineales:\n",
    "\n",
    "\\begin{align}\n",
    "x_0 + 2x_1 + x_2 &=& 4 \\\\\n",
    "x_0 + x_2 &=& 3 \\\\\n",
    "2x_0 + x_1 + \\frac{x_2}{4} &=& 3\n",
    "\\end{align}\n",
    "\n",
    "Este sistema puede ser escrito como producto de una matriz y un vector:\n",
    "\n",
    "$Ax = b$, con $A =  \\left[ \\begin{array} {ccc}\n",
    "1&2&1 \\\\\n",
    "1&0&1 \\\\\n",
    "2&1&1/4 \\end{array}\\right]$, $x= \\left[ \\begin{array}{c} x_0\\\\x_1\\\\x_2 \\end{array}\\right]$ y $b= \\left[ \\begin{array}{c} 4\\\\3\\\\3 \\end{array}\\right]$\n",
    "\n",
    "El álgebra lineal nos permite representar de forma más compacta los conjuntos de ecuaciones lineales y operar con ellos.\n",
    "\n",
    "_Algebra is the offer made by the devil to the mathematician. The devil says: I will give you this powerful machine, it will answer any question you like. All you need to do is give me your soul: give up geometry and you will have this marvelous machine. —Sir Michael Atiyah, 2002_\n",
    "\n",
    "### Notación\n",
    "\n",
    "- $A \\in \\mathbb{R}^{m\\times n}$ denota una matriz de valores reales, con $m$ filas y $n$ columnas.\n",
    "- $x \\in \\mathbb{R}^n$ es un vector de valores reales. Por convención, los vectores se representan como columnas (es decir, como una matriz de $\\mathbb{R}^{1 \\times n}$). Si por algún motivo queremos explícitamente denotar un vector fila, utilizaremos $x^T$, donde $x^T$ denota la matriz traspuesta de $x$, que definiremos enseguida. \n",
    "- El i-ésimo elemento de un vector $x$ se denota $x_i$\n",
    "- El elemento de una matriz $A$ ubicado en la i-ésima fila y en la j-ésima columna, lo denotamos $a_{ij}$ o $A_{ij}$ (nótese que, por convención, se indica primero la fila, y luego la columna). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices como transformaciones lineales\n",
    "\n",
    "De [Wikipedia](http://en.wikipedia.org/wiki/Matrix_%28mathematics%29): \n",
    "\n",
    "_A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three dimensional space is a linear transformation which can be represented by a rotation matrix R. If v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation._\n",
    "\n",
    "La siguiente imagen muesta como se transforma el cuadrado de la unidad bajo una transformación lineal representada por una matriz 2-por-2. Tal vez esta forma de visualizar nos ayude a entender mejor las propiedades que presentaremos. En este ejemplo, la matriz sería $\\left[ \\begin{array} {cc}\n",
    "a&c \\\\\n",
    "b&d \\\\ \\end{array}\\right]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/a/ad/Area_parallellogram_as_determinant.svg\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto de dos matrices\n",
    "\n",
    "El producto de dos matrices $A \\in \\mathbb{R}^{m\\times n}$ y $B \\in \\mathbb{R}^{m\\times p}$ es la matriz\n",
    "\n",
    "$C \\in \\mathbb{R}^{m\\times p}$,\n",
    "\n",
    "donde\n",
    "\n",
    "$$c_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}$$\n",
    "\n",
    "**El producto de dos matrices es una matriz que representa la composición de las dos transformaciones lineales denotadas por  las matrices originales**\n",
    "\n",
    "Cuando las dos matrices son vectores $x,y \\in \\mathbb{R}^n$, entonces $x^Ty$ (su _producto interno_) es un número real dado por \n",
    "\n",
    "$$ x^Ty \\in \\mathbb{R} = \\sum_{i=1}^{n} x_{i}y_{i}$$\n",
    "\n",
    "\n",
    "(y es consecuencia de aplicar directamente la definición, considerando a $x$ como un vector fila y a $y$ como un vector columna)\n",
    "\n",
    "Análogamente, la definición puede aplicarse para multiplicar matrices $A \\in \\mathbb{R}^{m\\times n}$ por vectores $x \\in \\mathbb{R}^n$. Obtenderemos un vector $y$ de $m$ elementos, donde cada elemento $y_i$ se corresponde al producto interno de la i-ésima fila de $A$ y $x$ (obsérvese que la cantidad de columnas de $A$ y de elementos de $x$ deben coincidir para que la operación esté definida). \n",
    "\n",
    "\n",
    "$$y =  \\left[ \\begin{array} {ccc}\n",
    "--& a_1^T&-- \\\\ \n",
    "--& a_2^T&-- \\\\ \n",
    "&\\vdots\\\\\n",
    "--& a_m^T & --  \\end{array}\\right] x = \n",
    "\\left[ \\begin{array} {ccc}\n",
    "--& a_1^Tx&-- \\\\ \n",
    "--& a_2^Tx&-- \\\\ \n",
    "&\\vdots\\\\\n",
    "--& a_m^Tx & -- \\end{array}\\right]$$\n",
    "\n",
    "Alternativamente, podemos ver a $y$ como una combinación lineal de las columnas de $A$, donde los coeficientes son los valores de $x$ (_Verificarlo!_):\n",
    "\n",
    "\n",
    "$$y =  \n",
    "\\left[ \\begin{array} {cccc}\n",
    "| &|& |& | \\\\ \n",
    "a_1 & a_2 & \\cdots & a_n \\\\ \n",
    "| &|& |& | \\end{array}\\right] \n",
    "\\left[ \\begin{array} {c}\n",
    "x_1 \\\\ \n",
    "x_2 \\\\\n",
    "\\vdots\\\\\n",
    "x_n\\end{array}\\right] \n",
    "= \n",
    "\\left[ \\begin{array} {c} a_1\\\\ \\end{array}\\right] x_1 +\n",
    "\\left[ \\begin{array} {c} a_2\\\\ \\end{array}\\right] x_2 + \\cdots +\n",
    "\\left[ \\begin{array} {c} a_n\\\\ \\end{array}\\right] x_n\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas definiciones\n",
    "\n",
    "- La matriz identidad es una matriz cuadrada ($I \\in \\mathbb{R}^{n \\times n}$) con valor 1 en la diagonal y los restantes elementos en 0. Se cumple que $ AI = A = IA $ (ajustando las dimensiones de I para que el producto sea posible). \n",
    "- La traspuesta de una matriz $A$ es la matriz resultante de intercambiar filas y columnas:\n",
    "\n",
    "$$ A^T \\in \\mathbb{R}^{n\\times m}, (A^T)_{ij} = a_{ji}$$ \n",
    "- Propiedades de la matriz traspuesta:\n",
    "    - $(A^T)^T = A$\n",
    "    - $(AB)^T = B^TA^T$\n",
    "    - $(A+B)^T = A^T+B^T$\n",
    "\n",
    "\n",
    "- Una matriz (cuadrada) $A^T \\in \\mathbb{R}^{n\\times n}$ es simétrica si $A = A^T$, y es antisimétrica si $A = -A^T$. Puede observarse que una matriz siempre puede expresarse como la suma de una matriz simétrica y otra antisimétrica:\n",
    "\n",
    "$$ A = \\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T) $$\n",
    "- La traza $tr(A)$ de una matriz cuadrada $A^T \\in \\mathbb{R}^{n\\times n}$ es la suma de los elementos en la diagonal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norma de un vector\n",
    "\n",
    "La norma de un vector $\\Vert v\\Vert$ es una medida del \"tamaño\" de un vector. Una norma $f: \\mathbb{R} \\to \\mathbb{R}$ debe satisfacer 4 propiedades:\n",
    "\n",
    "- Para todo $x \\in \\mathbb{R}^n, f(x) \\geq 0$\n",
    "- $f(x) = 0 $ sii $x=0$\n",
    "- Para todo $x \\in \\mathbb{R}^n, t \\in  \\mathbb{R}, f(tx)= |t|f(x)$\n",
    "- Para todo $x,y  \\in \\mathbb{R}^n, f(x+y) \\leq f(x)+f(y)$\n",
    "\n",
    "Normas comunes: \n",
    "\n",
    "- Norma euclídea ($\\ell_2$), equivalente a $\\sqrt{x^Tx}$ (puede verse que $||x||^2_2 = x^Tx$):\n",
    "\n",
    "$$ \\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$$\n",
    "\n",
    "- Norma $\\ell_1$:\n",
    "\n",
    "$$ \\Vert x \\Vert_1 = \\sum_{i=1}^n |x_i|$$\n",
    "\n",
    "- Norma $\\ell_\\infty$:\n",
    "\n",
    "$$ \\Vert x \\Vert_\\infty = max_i |x_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independencia Lineal\n",
    "\n",
    "Un conjunto de vectores es _linealmente independiente_ si ninguno de ellos puede ser representado como una combinación lineal de los otros. \n",
    "\n",
    "El _rango columna_ de una matriz $A$ es el mayor número de columnas de $A$ que constituyen un conjunto de vectores linealmente independientes. El _rango fila_ es el mayor número de filas de $A$ que constituyen un conjunto de vectores linealmente independientes. Se demuestra que, para toda matriz $A$, su rango columna es igual a su rango fila (y por lo tanto, hablamos en general de rango de una matriz). \n",
    "\n",
    "El rango de una matriz siempre es menor o igual al mínimo de filas o columnas de la matriz. Si $A \\in \\mathbb{R}^{m \\times n}$ y $rank(A) = min(m,n)$, decimos que $A$ es _full rank_. \n",
    "\n",
    "\n",
    "### Matriz inversa\n",
    "\n",
    "La inversa de una matriz cuadrada $A \\in \\mathbb{R}^{n \\times n}$, es la única matriz $A^{-1} \\in \\mathbb{R}^{n \\times n}$  que cumple\n",
    "\n",
    "$$ A^{-1}A = I = AA^{-1}$$\n",
    "\n",
    "Si $A^{-1}$ existe, entonces $A$ es _invertible_ (de lo contrario, es _no invertible_ o _singular_).\n",
    "\n",
    "Si $A,B \\in \\mathbb{R}^{n \\times n}$ son invertibles, entonces:\n",
    "\n",
    "- $(A^{-1})^{-1} = A$\n",
    "- Si $Ax = b$, entonces (multiplicando por $A^{-1}$ a ambos lados), obtenemos $x = A^{-1}b$ (lo que resuelve el sistema de ecuaciones!). \n",
    "- $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "- $ (A^{-1})^T = (A^T)^{-1}$ (también denotada $A^{-T}$)\n",
    "\n",
    "**Para transformaciones lineales, la matriz inversa representa la transformación lineal que nos devuelve el plano (en general el espacio) original**\n",
    "\n",
    "### Matrices ortogonales\n",
    "\n",
    "Dos vectores $x,y \\in \\mathbb{R}^n$ son _ortogonales_ si $x^Ty = 0$. Además, $x \\in \\mathbb{R}^n$ está _normalizado_ si $\\Vert x \\Vert_2 = 1$. Una matriz cuadrada $U$ es _ortogonal_ si sus columnas son ortogonales entre sí, y están normalizadas (es decir, son _ortonormales_).\n",
    "\n",
    "De la definición surge que la inversa de una matriz ortogonal es su traspuesta:\n",
    "\n",
    "$$ U^TU = I = UU^T$$\n",
    "\n",
    "Es interesante ver que multiplicar un vector por una matriz ortogonal no modifica su norma Euclídea:\n",
    "\n",
    "$$ ||Ux||^2_2 = (Ux)^TUx=x^TU^TUx = x^Tx = ||x||^2$$\n",
    "\n",
    "### Span  y Nullspace\n",
    "\n",
    "Dado un conjunto de vectores $C=\\{x_1,\\ldots, x_n\\}$, su _span_ es el conjunto de los vectores que pueden expresarse como combinación lineal de $C$. Si los vectores de $C$ son linealmente independientes y $x_i \\in C$, entonces su span es $\\mathbb{R}^{n}$. La _proyección_ de un vector $ y \\in \\mathbb{R}^{m}$ en el span de $\\{x_1,\\ldots, x_n\\}$, con $x_i \\in \\mathbb{R}^{m}$, es el vector $v$ cuya norma $||v-y||_2$ es mínima.\n",
    "\n",
    "\n",
    "El _rango_ $R(A)$ de una matriz es el span de las columnas de A. Si $A$ es full rank y $n < m$, entonces la proyección de un vector $y \\in \\mathbb{R}^m$ sobre el rango de $A$ está dado por:\n",
    "\n",
    "$$ A(A^TA)^{-1}A^Ty $$\n",
    "\n",
    "El _nullspace_ $N(A)$ de una matriz es el conjunto de vectores que dan como resultado 0 cuando son multiplicados por $A$. Puede verse que $R(A)$ y $N(A)$ generan $\\mathbb{R}^n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinante de una matriz\n",
    "\n",
    "El _determinante_ de una matriz cuadrada $A \\in \\mathbb{R}^{n \\times n}$, denotado $|A|$ es una función que cumple:\n",
    "\n",
    "1. $|I|  = 1$\n",
    "2. Si multiplicamos una fila de la matriz por un escalar $t \\in \\mathbb{R}$, se cumple que el determinante de la matriz obtenida es $ t|A|$\n",
    "3. Si intercambiamos dos filas de $A$, el detrminante de la nueva matriz es $-|A|$.\n",
    "\n",
    "A partir de esta definición, se cumplen ciertas propiedades:\n",
    "\n",
    "- Para $A \\in \\mathbb{R}^{n \\times n}$ $|A| = |A^T|$\n",
    "- Para $A,B \\in \\mathbb{R}^{n \\times n}$, $|AB| = |A||B|$\n",
    "- Para $A \\in \\mathbb{R}^{n \\times n}$,  $|A| = 0$ sii $A$ es no invertible\n",
    "- Para $A \\in \\mathbb{R}^{n \\times n}$ invertible,  $|A^{-1}| = 1/|A|$ \n",
    "\n",
    "De esta forma, para saber si una matriz es invertible o no, basta con calcular su determinante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formas cuadráticas\n",
    "\n",
    "Dada una matriz cuadrada $A \\in \\mathbb{R}^{n \\times n}$ y un vector $x \\in\\mathbb{R}^n$, el valor _escalar_ $x^TAx$ es llamado _forma cuadrática_ (y es un polinomio donde todos sus términos tienen valor 2).\n",
    "\n",
    "Podemos ver que:\n",
    "\n",
    "$$ x^TAx = (x^TAx)^T = x^TA^Tx = x^T(\\frac{1}{2}A + \\frac{1}{2}A^T)x $$ \n",
    "\n",
    "Por lo que una forma cuadrática solamente utiliza la parte simétrica de $A$ (recordando que una matriz siempre puede verse como la suma de una matriz simétrica y una antisimétrica). Por lo tanto, es común asumir que las matrices que aparecen en una forma cuadrática son simétricas.\n",
    "\n",
    "\n",
    "- Dada una matriz simétrica, decimos que $A$ es _positiva (semi) definida_ si para todo vector no nulo $x \\in \\mathbb{R}^n$, $x^TAx \\gt 0$ ( $x^TAx \\ge 0$)\n",
    "\n",
    "- Análogamente $A$ es _negativa (semi) definida_ si para todo vector no nulo $x \\in \\mathbb{R}^n$, $x^TAx \\lt 0$ ( $x^TAx \\le 0$)\n",
    "\n",
    "Las matrices positivas definidas y negativas definidas son siempre invertibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores y vectores propios\n",
    "\n",
    "Si $A \\in \\mathbb{R}^{n \\times n}$, decimos que $\\lambda \\in \\mathbb{C}$ es un _valor propio_ y que $x \\in \\mathbb{C}^n$ es su _vector propio correspondiente_ si: \n",
    "\n",
    "$$Ax = \\lambda x, x \\neq 0$$\n",
    "\n",
    "Visto como transformación lineal, multiplicar $A$ por un vector propio $x$ resulta en un nuevo vector, con la misma dirección, pero escalado por un factor $\\lambda$. En la siguiente figura (tomada de Wikipedia), se muestra la transformación producida por la matriz $\\left[ \\begin{array} {cc}2&1 \\\\1&2 \\end{array}\\right]$. Los vectores azules y rosados son los vectores propios de la matriz. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/a/ad/Eigenvectors-extended.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un vector propio (o _eigenvector_) $x \\in \\mathbb{C}$ de $A$, y un escalar $t \\in \\mathbb{C}$, entonces $tx$ también es un vector propio de $A$. En general, asumimos como representante canónico de esta clase al vector propio de largo 1 (normalizado). \n",
    "\n",
    "No entraremos aquí en la forma de calcular valores y vectores propios, pero sí enunciaremos algunas propiedades:\n",
    "\n",
    "- La traza de una matriz $A$ es igual a la suma de sus valores propios.\n",
    "- El determinante de una matriz $A$ es el producto de sus valores propios.\n",
    "- El rango de $A$ es el número de valores propios no nulos.\n",
    "- Si $A$ es invertible, entonces $1/\\lambda_i$ es el valor propio de $A^{-1}$, con vector propio asociado $x_i$.\n",
    "- Los valores propios de una matriz diagonal (aquella que solamente tiene valores no negativos en su diagonal principal) son los valores de la diagonal.\n",
    "\n",
    "Si construimos $X$ como la matriz cuyas columnas son los vectores propios $x_i$ de $A$, entonces tendremos (por la definición de valor y vector propio):\n",
    "\n",
    "$$ AX = \\left[ \\begin{array} {cccc}\n",
    "\\lambda_1 x_1& \\lambda_2 x_2&\\ldots& \\lambda_n x_n \\end{array}\\right]$$ \n",
    "\n",
    "donde $\\lambda_i$ es el $i$-ésimo valor propio de A. Podemos entonces escribir todas las ecuaciones de los valores y vectores propios de la siguiente forma: \n",
    "\n",
    "$$ AX = X \\Lambda$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "donde $\\Lambda$ es una matriz diagonal cuyas entradas son los valores propios de $A$. \n",
    "\n",
    "Entonces, si los vectores propios de A son linealmente independientes, $X$ será invertible y por lo tanto $A = X\\Lambda X^{-1}$ y diremos que A es _diagonalizable_.\n",
    "\n",
    "Esto quiere decir que $A$ puede descomponerse en una matriz compuesta de sus vectores propios, una matriz con sus valores propios en la diagonal, y la inversa de su matriz de vectores propios. Esencialmente, esto quiere decir que $A$ y $\\Lambda$ representan la misma transformación lineal expresada en dos bases diferentes, siendo $X$ la matriz de cambio de base. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectores y valores propios de matrices simétricas \n",
    "\n",
    "Supongamos que $A \\in \\mathbb{R^{n \\times n}}$ es simétrica. Entonces:\n",
    "\n",
    "- Todos sus valores propios son reales.\n",
    "- Sus vectores propios son ortonormales, por lo que $X$ será ortogonal (y la llamaremos $U$). \n",
    "\n",
    "Entonces $A = U\\Lambda U^{T}$ (porque la inversa de una matriz ortogonal es su traspuesta), y se puede demostrar que $A$ es positiva definida si todos sus valores propios son mayores que 0 (y semidefinida si son mayores o iguales a 0), y negativa definida si todos sus valores propios son menores que 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente de una matriz y un vector\n",
    "\n",
    "El cálculo matemático puede extenderse bastante directamente a vectores y matrices. En esta sección se presentan el gradiente (\"equivalente\" vectorial de las derivada primera del cálculo de una variable). Dada una función $f: \\mathbb{R}^{m \\times n} \\to \\mathbb{R}$ que recibe una matriz y devuelve un valor real, su _gradiente_ con respecto a $A$ es la matriz de derivadas parciales respecto a cada uno de los componentes de la matriz, definida como:\n",
    "\n",
    "$$ \\nabla_A f(A) \\in \\mathbb{R}^{m \\times n} = \\left[ \n",
    "\\begin{array}{cccc} \n",
    "\\frac{\\partial f(A)}{\\partial{A_{11}}}& \\frac{\\partial f(A)}{\\partial{A_{12}}} & \\cdots & \\frac{\\partial f(A)}{\\partial{A_{1n}}}  \\\\ \n",
    "\\frac{\\partial f(A)}{\\partial{A_{21}}}& \\frac{\\partial f(A)}{\\partial{A_{22}}} & \\cdots & \\frac{\\partial f(A)}{\\partial{A_{2n}}}  \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots  \\\\ \n",
    "\\frac{\\partial f(A)}{\\partial{A_{m1}}}& \\frac{\\partial f(A)}{\\partial{A_{m2}}} & \\cdots & \\frac{\\partial f(A)}{\\partial{A_{mn}}}\\end{array}\\right ] $$\n",
    "\n",
    "En el caso particular de un vector $x \\in \\mathbb{R}^{n}$ , tendremos:\n",
    "\n",
    "$$ \\nabla_x f(x) \\in \\mathbb{R}^{m} = \\left[ \n",
    "\\begin{array}{c} \n",
    "\\frac{\\partial f(x)}{\\partial{x_{1}}}\\\\ \n",
    "\\frac{\\partial f(x)}{\\partial{x_{2}}}\\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial f(x)}{\\partial{x_{m}}}\n",
    "\\end{array}\\right ] $$\n",
    "\n",
    "\n",
    "Observemos que la función retorna un valor escalar (no un vector). De la definición surge que el gradiente de la suma de dos funciones es la suma de sus gradientes, y que el gradiente de la multiplicación de un escalar por el resultado de una función es equivalente a multiplicar el gradiente de la función por el escalar. \n",
    "\n",
    "La _matriz hessiana_  de una función $f: \\mathbb{R}^{ n} \\to \\mathbb{R}$ respecto a $x$, $\\nabla^2_x f(x)$ es la matriz $n \\times n$ de derivadas parciales:\n",
    "\n",
    "$$ (\\nabla^2_x f(x))_{ij} = \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}$$\n",
    "\n",
    "La hessiana es el \"equivalente\" de la segunda derivada, en el siguiente sentido: \n",
    "\n",
    "$$ \\nabla^2_x f(x) = [ \\nabla_x (\\nabla_x f(x))_1 \\;  \\nabla_x (\\nabla_x f(x))_2  \\;  \\ldots \\; \\nabla_x (\\nabla_x f(x))_n] $$\n",
    "\n",
    "es decir, tomamos el gradiente respecto a $x$ de la i-ésima fila del gradiente $(\\nabla_x f(x))_i = \\partial f(x)/\\partial x_i$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente y Hessiano  de una función lineal\n",
    "\n",
    "Si $f(x) = b^Tx$ para $x \\in  \\mathbb{R}^n$, tendremos (demostrar!)\n",
    "\n",
    "$$ \\frac{\\partial f(x)}{\\partial x_k} = b_k$$\n",
    "\n",
    "Si tenemos una función cuadrática $f(x) = x^TAx$ para $A \\in \\mathbb{S}^n$ simétrica. Entonces:\n",
    "\n",
    "$$ \\nabla_x x^TAx = 2Ax$$\n",
    "\n",
    "Finalmente, su Hessiana es:\n",
    "\n",
    "$$ \\nabla_x^2 x^TAx = 2A$$\n",
    "\n",
    "(Nótese que los resultados son análogos a los casos de una sola variable...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación: Ecuaciones normales para mínimos cuadrados\n",
    "\n",
    "Supongamos que tenemos una matriz $A \\in \\mathbb{R}^{m \\times n}$ (full rank) y un vector $b \\in \\mathbb{R}^{m}$, tal que $b \\not\\in R(A)$. No es posible aquí encontrar un $x \\in \\mathbb{R}^{n}$ tal que $Ax =b$, por lo que vamos a buscar un vector tal que $Ax$ quede tan cerca como sea posible a $b$ (como medida de distancia utilizaremos la norma Euclídea $||Ax -h||^2_2$).\n",
    "\n",
    "Como $||x ||^2_2 = x^T x$, tenemos:\n",
    "\n",
    "$$ || Ax -b ||^2_2 = (Ax -b)^T (Ax -b) = x^TA^TAx - 2b^TAx +b^Tb$$\n",
    "\n",
    "Si tomamos el gradiente obtenemos:\n",
    "\n",
    "$$ \\nabla_x (x^TA^TAx - 2b^TAx + b^Tb) = \\nabla_x x^TA^TAx - \\nabla_x 2b^TAx - \\nabla_xb^Tb = 2A^TAx - 2A^Tb$$\n",
    "\n",
    "Si igualamos el valor del gradiente a 0, y despejamos x, obtenemos (mostrarlo!):\n",
    "\n",
    "$$ x = (A^TA)^{-1}A^Tb $$\n",
    "\n",
    "Obsérvese que hemos obtenido una solución cerrada al problema de ajustar una curva minimizando la distancia de mínimos cuadrados, un método publicado por primera vez en 1805 por Adrien-Marie Legendre, pero que muchos atribuyen a Carl Friedrich Gauss (1795). En nuestro curso, utilizaremos una solución alternativa, que utiliza aproximaciones numéricas (descenso por gradiente). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adenda: la intuiciòn detrás de algunos conceptos\n",
    "\n",
    "Una interpretación geométrica tomado de StackExchange: http://math.stackexchange.com/q/636198\n",
    "\n",
    "**El determinante es un factor de cambio de volumen**\n",
    "\n",
    "Si pensamos en una matriz como en una transformación geométrica que mapea puntos (vectores columna) a puntos, el determinante det(M) nos da el factor por el cual cambia el volumen bajo este mapeo. \n",
    "\n",
    "**Una matriz mapea una esfera a una elipsoide**\n",
    "\n",
    "Al ser una transformación lineal, una matriz mapea una esfera a una elipsoide. La descomposición en valores singulares lo muestra claramente. Si se considera el eje principal del elipsoide (y su preimagen en la esfera), la descomposición expresas la matriz como del producto de (1) una rotación que alinea el ejem principal con el de las coordinadas, (2) escala en la dirección de las coordenadas para obtener la forma elipsoidal, y (3) vuelve a rotar hasta la posición final. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
